{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnCSJ841/E/LLcvtJiqQKG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"593UbmyHZFsM"},"source":["# Fully-Connected Neural Network"]},{"cell_type":"markdown","metadata":{"id":"kr7AycivZNyG"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"id":"OZZQ4RXxZPRt"},"source":["# Import Necessary Packages and Mount Drive\n","!pip install scikit-learn --upgrade\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","from math import sqrt\n","from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","pd.set_option('display.max_columns', None)\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)\n","%cd '/content/gdrive/MyDrive/ECSE_552/Project'\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.utils.data import random_split\n","from torchvision import transforms\n","\n","!pip install pytorch_lightning\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.utilities import rank_zero_only\n","from pytorch_lightning.loggers import LightningLoggerBase\n","from pytorch_lightning.loggers.base import rank_zero_experiment\n","from collections import defaultdict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbBTz_EpZPN5"},"source":["# Create checkpoint callback that monitors validation loss\n","class LitAutoEncoder(pl.LightningModule):\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.backbone(x)\n","\n","        # 1. calculate loss\n","        loss = F.cross_entropy(y_hat, y)\n","\n","        # 2. log `val_loss`\n","        self.log('val_loss', loss)\n","\n","\n","# Logger\n","class DictLogger(LightningLoggerBase):\n","\n","    def __init__(self):\n","        super().__init__()\n","        def def_value(): \n","            return []\n","              \n","        # Defining the dict \n","        self.metrics = defaultdict(def_value) \n","\n","\n","    @property\n","    def name(self):\n","        return 'DictLogger'\n","\n","    @property\n","    @rank_zero_experiment\n","    def experiment(self):\n","        # Return the experiment object associated with this logger.\n","        pass\n","\n","    @property\n","    def version(self):\n","        # Return the experiment version, int or str.\n","        return '0.1'\n","\n","    @rank_zero_only\n","    def log_hyperparams(self, params):\n","        # params is an argparse.Namespace\n","        # your code to record hyperparameters goes here\n","        pass\n","\n","    @rank_zero_only\n","    def log_metrics(self, metrics, step):\n","        # metrics is a dictionary of metric names and values\n","        # your code to record metrics goes here\n","        for key in metrics.keys():\n","            self.metrics[key].append(metrics[key])\n","\n","    @rank_zero_only\n","    def save(self):\n","        # Optional. Any code necessary to save logger data goes here\n","        # If you implement this, remember to call `super().save()`\n","        # at the start of the method (important for aggregation of metrics)\n","        super().save()\n","\n","    @rank_zero_only\n","    def finalize(self, status):\n","        # Optional. Any code that needs to be run after training\n","        # finishes goes here\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAIhejexYb2R"},"source":["# RMSELoss\n","class RMSELoss(nn.Module):\n","    def __init__(self, eps=1e-10):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","        self.eps = eps\n","        \n","    def forward(self,yhat,y):\n","        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wA9OnpGroUGA"},"source":["# Import the dataset\n","df = pd.read_csv('data/ON_demand_weather_17-20.csv', index_col=0)\n","\n","\n","# Do all the preprocessing required\n","from preprocessing.preprocessing import one_hot_encode_days, one_hot_encode_months, time_differencing, split_and_scale, sin_cos_waves, sliding_windows\n","\n","\n","# Remove weather stations and market demand columns\n","df.drop(['Market Demand',\n","         'toronto_Temp (C)',\n","         'hamilton_Temp (C)',\n","         'ottawa_Temp (C)',\n","         'kitchener_Temp (C)',\n","         'london_Temp (C)',\n","         'windsor_Temp (C)'], axis=1, inplace=True)\n","\n","# Apply scaling if desired and split dataset into train, validation, and testset\n","scaler = 'standard'  # Define the type of scaler, options are 'min-max', 'standard', and None\n","columns_to_scale = ['Ontario Demand', 'Weighted Average Temp (C)']  # Define columns to be scaled, add time-differenced column if applicable\n","target_column = columns_to_scale[0]  # Define target column (required to allow unscaling later), change to time-differenced column if applicable\n","vali_set_start_date='2019-07-01'  # First day of validation set\n","test_set_start_date='2020-01-01'  # First day of test set\n","\n","train_df, vali_df, test_df, target_scaler = split_and_scale(df,\n","                                                            scaler=scaler,\n","                                                            columns_to_scale=columns_to_scale,\n","                                                            target_column=target_column,\n","                                                            vali_set_start_date=vali_set_start_date,\n","                                                            test_set_start_date=test_set_start_date)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbbuOz7XpVmr"},"source":["## Model Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"SXI7SA8XpbbB"},"source":["#### 1Hour Prediction"]},{"cell_type":"code","metadata":{"id":"--9dRcn9pugp"},"source":["# Create sliding window for 1H predictions\n","window_size = 24\n","flatten = True\n","output_window_size = 1\n","perform_feature_shift = False\n","\n","# Sliding windows\n","# note that the function removes the 'Date' column (not numeric)\n","x_train, y_train = sliding_windows(df=train_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_vali, y_vali = sliding_windows(df=vali_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_test, y_test = sliding_windows(df=test_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","\n","# Adjust target vectors, keep only last element\n","y_train = y_train[:, -1].reshape(-1,1)\n","y_vali = y_vali[:, -1].reshape(-1,1)\n","\n","BATCH_SIZE = 32\n","# Convert to tensor\n","x_train_tensor = torch.FloatTensor(x_train)\n","y_train_tensor = torch.FloatTensor(y_train)\n","x_vali_tensor = torch.FloatTensor(x_vali)\n","y_vali_tensor = torch.FloatTensor(y_vali)\n","x_test_tensor = torch.FloatTensor(x_test)\n","y_test_tensor = torch.FloatTensor(y_test)\n","\n","# Create Dataset\n","train_data = TensorDataset(x_train_tensor, y_train_tensor)\n","valid_data = TensorDataset(x_vali_tensor, y_vali_tensor)\n","test_data = TensorDataset(x_test_tensor, y_test_tensor)\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)\n","test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXsf6RnPoTwK"},"source":["# Hyperparams\n","n_inputs = x_train.shape[1]\n","output_size = 1\n","n_neuron = 100\n","max_epochs = 200\n","\n","# Lightning Module\n","class fcnn(pl.LightningModule):\n","  def __init__(self, n_inputs, hidden_layer_size, output_size):\n","    super().__init__()\n","    self.n_inputs = n_inputs\n","    self.output_size = output_size\n","\n","    self.input_layer = torch.nn.Linear(n_inputs, hidden_layer_size)\n","    self.layer_h1 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    self.output_layer = torch.nn.Linear(hidden_layer_size, output_size)\n","\n","  def forward(self, x):\n","    batch_size, n_features = x.size()\n","    x = x.view(batch_size, -1)\n","    x = F.relu(self.input_layer(x))\n","    x = F.relu(self.layer_h1(x))\n","    # x = F.relu(self.layer_h2(x))\n","    # x = F.relu(self.layer_h3(x))\n","    output = self.output_layer(x)\n","    return output\n","  def configure_optimizers(self):\n","    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, min_lr=1e-7)\n","    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n","\n","  def training_step(self, train_batch, batch_idx):\n","    x, y = train_batch\n","    x_hat = self(x)\n","    train_loss = F.mse_loss(x_hat, y)\n","    self.log('train_loss', train_loss, on_step=False, on_epoch=True)\n","    return train_loss\n","  def validation_step(self, val_batch, batch_idx):\n","    x, y = val_batch\n","    x_hat = self(x)\n","    val_loss = F.mse_loss(x_hat, y)\n","    self.log('val_loss', val_loss)\n","\n","\n","fcnn_model = fcnn(n_inputs, n_neuron, output_size)\n","\n","# training\n","# Init ModelCheckpoint callback, monitoring 'val_loss'\n","checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n","\n","# Learning rate monitor\n","lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","fcnn_logger = DictLogger()\n","early_stopping = EarlyStopping('val_loss', verbose=True, min_delta=0.0001, patience=5)\n","trainer = pl.Trainer(max_epochs=max_epochs, gpus=1, progress_bar_refresh_rate=30, logger=fcnn_logger, callbacks=[early_stopping,checkpoint_callback, lr_monitor],\n","                    default_root_dir='/content/gdrive/MyDrive/ECSE_552/Project/Checkpoints/fcnn')\n","start_time = time.time()\n","trainer.fit(fcnn_model, train_dataloader, valid_dataloader)\n","fcnn_training_time = time.time() - start_time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erITZqvHqGH7"},"source":["##### Test"]},{"cell_type":"code","metadata":{"id":"o1kxEiXUqHbo"},"source":["dataset = valid_dataloader\n","if target_scaler is not None:\n","  for x,y in dataset:\n","    x_data = x\n","    y_data = y\n","\n","  train_pred = fcnn_model(x_train_tensor).detach()\n","  pred = fcnn_model(x_data).detach()\n","\n","  train_unscaled_pred = target_scaler.inverse_transform(train_pred)\n","  train_unscaled_y_data = target_scaler.inverse_transform(y_train_tensor)\n","\n","  unscaled_pred = target_scaler.inverse_transform(pred)\n","  unscaled_y_data = target_scaler.inverse_transform(y_data)\n","\n","  train_loss_unscaled = sqrt(mean_squared_error(train_unscaled_y_data, train_unscaled_pred))\n","  vali_loss_unscaled = sqrt(mean_squared_error(unscaled_y_data, unscaled_pred))\n","\n","  print('Unscaled Train RMSE: {:.3f}'.format(train_loss_unscaled))\n","  print('Unscaled Vali RMSE: {:.3f}'.format(vali_loss_unscaled))\n","\n","dataset = test_dataloader\n","if target_scaler is not None:\n","  for x,y in dataset:\n","    x_data = x\n","    y_data = y\n","\n","\n","  pred = fcnn_model(x_data).detach()\n","\n","  unscaled_pred = target_scaler.inverse_transform(pred)\n","  unscaled_y_data = target_scaler.inverse_transform(y_data)\n","\n","  test_loss_unscaled = sqrt(mean_squared_error(unscaled_y_data, unscaled_pred))\n","  test_mape = mean_absolute_percentage_error(unscaled_y_data, unscaled_pred)\n","\n","\n","  print('Unscaled Test RMSE: {:.3f}'.format(test_loss_unscaled))\n","  print('Test MAPE: {:.4f}'.format(test_mape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P8Q3L9jXqI6p"},"source":["# Plot RF performance\n","plt.figure(figsize=(10,2))\n","plt.plot(unscaled_pred[1:25], label='pred')\n","plt.plot(unscaled_y_data[1:25], label='true')\n","plt.legend()\n","plt.xlabel('hours')\n","plt.ylabel('Demand in MW')\n","plt.title('FCNN prediction example on validation set')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKHbjDmQqUlF"},"source":["#### 24Hour Sequence Prediction "]},{"cell_type":"code","metadata":{"id":"CI4FZfqArW9Y"},"source":["# Create sliding window for 1H predictions\n","window_size = 24\n","flatten = True\n","output_window_size = 24\n","perform_feature_shift = False\n","\n","# Sliding windows\n","# note that the function removes the 'Date' column (not numeric)\n","x_train, y_train = sliding_windows(df=train_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_vali, y_vali = sliding_windows(df=vali_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_test, y_test = sliding_windows(df=test_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","\n","# Adjust target vectors, keep only last element\n","y_train = y_train[:, -1].reshape(-1,1)\n","y_vali = y_vali[:, -1].reshape(-1,1)\n","\n","BATCH_SIZE = 32\n","# Convert to tensor\n","x_train_tensor = torch.FloatTensor(x_train)\n","y_train_tensor = torch.FloatTensor(y_train)\n","x_vali_tensor = torch.FloatTensor(x_vali)\n","y_vali_tensor = torch.FloatTensor(y_vali)\n","x_test_tensor = torch.FloatTensor(x_test)\n","y_test_tensor = torch.FloatTensor(y_test)\n","\n","# Create Dataset\n","train_data = TensorDataset(x_train_tensor, y_train_tensor)\n","valid_data = TensorDataset(x_vali_tensor, y_vali_tensor)\n","test_data = TensorDataset(x_test_tensor, y_test_tensor)\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)\n","test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LfxXaZs0rWd1"},"source":["# Hyperparams\n","n_inputs = x_train.shape[1]\n","output_size = 1\n","n_neuron = 100\n","max_epochs = 200\n","\n","# Lightning Module\n","class fcnn(pl.LightningModule):\n","  def __init__(self, n_inputs, hidden_layer_size, output_size):\n","    super().__init__()\n","    self.n_inputs = n_inputs\n","    self.output_size = output_size\n","\n","    self.input_layer = torch.nn.Linear(n_inputs, hidden_layer_size)\n","    self.layer_h1 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    self.output_layer = torch.nn.Linear(hidden_layer_size, output_size)\n","\n","  def forward(self, x):\n","    batch_size, n_features = x.size()\n","    x = x.view(batch_size, -1)\n","    x = F.relu(self.input_layer(x))\n","    x = F.relu(self.layer_h1(x))\n","    # x = F.relu(self.layer_h2(x))\n","    # x = F.relu(self.layer_h3(x))\n","    output = self.output_layer(x)\n","    return output\n","  def configure_optimizers(self):\n","    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, min_lr=1e-7)\n","    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n","\n","  def training_step(self, train_batch, batch_idx):\n","    x, y = train_batch\n","    x_hat = self(x)\n","    train_loss = F.mse_loss(x_hat, y)\n","    self.log('train_loss', train_loss, on_step=False, on_epoch=True)\n","    return train_loss\n","  def validation_step(self, val_batch, batch_idx):\n","    x, y = val_batch\n","    x_hat = self(x)\n","    val_loss = F.mse_loss(x_hat, y)\n","    self.log('val_loss', val_loss)\n","\n","\n","fcnn_model = fcnn(n_inputs, n_neuron, output_size)\n","\n","# training\n","# Init ModelCheckpoint callback, monitoring 'val_loss'\n","checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n","\n","# Learning rate monitor\n","lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","fcnn_logger = DictLogger()\n","early_stopping = EarlyStopping('val_loss', verbose=True, min_delta=0.0001, patience=5)\n","trainer = pl.Trainer(max_epochs=max_epochs, gpus=1, progress_bar_refresh_rate=30, logger=fcnn_logger, callbacks=[early_stopping,checkpoint_callback, lr_monitor],\n","                    default_root_dir='/content/gdrive/MyDrive/ECSE_552/Project/Checkpoints/fcnn')\n","start_time = time.time()\n","trainer.fit(fcnn_model, train_dataloader, valid_dataloader)\n","fcnn_training_time = time.time() - start_time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfNIF-LWrnyh"},"source":["##### Test"]},{"cell_type":"code","metadata":{"id":"TltJvTaSrWZU"},"source":["dataset = valid_dataloader\n","if target_scaler is not None:\n","  for x,y in dataset:\n","    x_data = x\n","    y_data = y\n","\n","  train_pred = fcnn_model(x_train_tensor).detach()\n","  pred = fcnn_model(x_data).detach()\n","\n","  train_unscaled_pred = target_scaler.inverse_transform(train_pred)\n","  train_unscaled_y_data = target_scaler.inverse_transform(y_train_tensor)\n","\n","  unscaled_pred = target_scaler.inverse_transform(pred)\n","  unscaled_y_data = target_scaler.inverse_transform(y_data)\n","\n","  train_loss_unscaled = sqrt(mean_squared_error(train_unscaled_y_data, train_unscaled_pred))\n","  vali_loss_unscaled = sqrt(mean_squared_error(unscaled_y_data, unscaled_pred))\n","\n","  print('Unscaled Train RMSE: {:.3f}'.format(train_loss_unscaled))\n","  print('Unscaled Vali RMSE: {:.3f}'.format(vali_loss_unscaled))\n","\n","dataset = test_dataloader\n","if target_scaler is not None:\n","  for x,y in dataset:\n","    x_data = x\n","    y_data = y\n","\n","\n","  pred = fcnn_model(x_data).detach()\n","\n","  unscaled_pred = target_scaler.inverse_transform(pred)\n","  unscaled_y_data = target_scaler.inverse_transform(y_data)\n","\n","  test_loss_unscaled = sqrt(mean_squared_error(unscaled_y_data, unscaled_pred))\n","  test_mape = mean_absolute_percentage_error(unscaled_y_data, unscaled_pred)\n","\n","\n","  print('Unscaled Test RMSE: {:.3f}'.format(test_loss_unscaled))\n","  print('Test MAPE: {:.4f}'.format(test_mape))\n","  \n","# Get last value of label sequence\n","last_true_value = y_vali[:,-1]\n","\n","# Get last value of predicted sequence\n","vali_pred = fcnn_model(x_vali_tensor).detach()\n","last_pred_value = vali_pred[:,-1]\n","\n","# Unscale\n","if scaler is not None:\n","  last_pred_value_unscaled = target_scaler.inverse_transform(last_pred_value)\n","  last_true_value_unscaled = target_scaler.inverse_transform(last_true_value)\n","\n","# Compute loss on last prediction\n","last_loss = mean_squared_error(last_true_value, last_pred_value)\n","print('RMSE on last time step of predicted sequence: {:.3f}'.format(sqrt(last_loss)))\n","if scaler is not None:\n","  last_loss_unscaled = mean_squared_error(last_true_value_unscaled, last_pred_value_unscaled)\n","  print('RMSE (unscaled) on last time step of predicted sequence: {:.3f}'.format(sqrt(last_loss_unscaled)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lW44S8XUrWUK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otYSHbrsrzeP"},"source":["#### 24Hour AutoRegressive\n","The 24H AutoRegressive Model for FCNN is based on the 1H model."]},{"cell_type":"code","metadata":{"id":"PQBkxzN5rzeR"},"source":["# Create sliding window for 1H predictions\n","window_size = 24\n","flatten = True\n","output_window_size = 1\n","perform_feature_shift = False\n","\n","# Sliding windows\n","# note that the function removes the 'Date' column (not numeric)\n","x_train, y_train = sliding_windows(df=train_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_vali, y_vali = sliding_windows(df=vali_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","x_test, y_test = sliding_windows(df=test_df, window_size=window_size, target_column=target_column, flatten=flatten, output_window_size=output_window_size, perform_feature_shift=perform_feature_shift)\n","\n","# Adjust target vectors, keep only last element\n","y_train = y_train[:, -1].reshape(-1,1)\n","y_vali = y_vali[:, -1].reshape(-1,1)\n","\n","BATCH_SIZE = 32\n","# Convert to tensor\n","x_train_tensor = torch.FloatTensor(x_train)\n","y_train_tensor = torch.FloatTensor(y_train)\n","x_vali_tensor = torch.FloatTensor(x_vali)\n","y_vali_tensor = torch.FloatTensor(y_vali)\n","x_test_tensor = torch.FloatTensor(x_test)\n","y_test_tensor = torch.FloatTensor(y_test)\n","\n","# Create Dataset\n","train_data = TensorDataset(x_train_tensor, y_train_tensor)\n","valid_data = TensorDataset(x_vali_tensor, y_vali_tensor)\n","test_data = TensorDataset(x_test_tensor, y_test_tensor)\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)\n","test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZrUebHirzeT"},"source":["# Hyperparams\n","n_inputs = x_train.shape[1]\n","output_size = 1\n","n_neuron = 100\n","max_epochs = 200\n","\n","# Lightning Module\n","class fcnn(pl.LightningModule):\n","  def __init__(self, n_inputs, hidden_layer_size, output_size):\n","    super().__init__()\n","    self.n_inputs = n_inputs\n","    self.output_size = output_size\n","\n","    self.input_layer = torch.nn.Linear(n_inputs, hidden_layer_size)\n","    self.layer_h1 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h2 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    # self.layer_h3 = torch.nn.Linear(hidden_layer_size, hidden_layer_size)\n","    self.output_layer = torch.nn.Linear(hidden_layer_size, output_size)\n","\n","  def forward(self, x):\n","    batch_size, n_features = x.size()\n","    x = x.view(batch_size, -1)\n","    x = F.relu(self.input_layer(x))\n","    x = F.relu(self.layer_h1(x))\n","    # x = F.relu(self.layer_h2(x))\n","    # x = F.relu(self.layer_h3(x))\n","    output = self.output_layer(x)\n","    return output\n","  def configure_optimizers(self):\n","    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, min_lr=1e-7)\n","    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n","\n","  def training_step(self, train_batch, batch_idx):\n","    x, y = train_batch\n","    x_hat = self(x)\n","    train_loss = F.mse_loss(x_hat, y)\n","    self.log('train_loss', train_loss, on_step=False, on_epoch=True)\n","    return train_loss\n","  def validation_step(self, val_batch, batch_idx):\n","    x, y = val_batch\n","    x_hat = self(x)\n","    val_loss = F.mse_loss(x_hat, y)\n","    self.log('val_loss', val_loss)\n","\n","\n","fcnn_model = fcnn(n_inputs, n_neuron, output_size)\n","\n","# training\n","# Init ModelCheckpoint callback, monitoring 'val_loss'\n","checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n","\n","# Learning rate monitor\n","lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","fcnn_logger = DictLogger()\n","early_stopping = EarlyStopping('val_loss', verbose=True, min_delta=0.0001, patience=5)\n","trainer = pl.Trainer(max_epochs=max_epochs, gpus=1, progress_bar_refresh_rate=30, logger=fcnn_logger, callbacks=[early_stopping,checkpoint_callback, lr_monitor],\n","                    default_root_dir='/content/gdrive/MyDrive/ECSE_552/Project/Checkpoints/fcnn')\n","start_time = time.time()\n","trainer.fit(fcnn_model, train_dataloader, valid_dataloader)\n","fcnn_training_time = time.time() - start_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4cZ6ZaysGMS"},"source":["# AutoRegressive Part\n","pred_length = 24  # number of steps done per prediction\n","dataset = test_df.copy()  # define dataset (before sliding window)\n","dataset.drop(columns='Date', inplace=True)\n","dataset[target_column] = dataset.pop(target_column)\n","dataset = torch.FloatTensor(dataset.to_numpy())\n","\n","# Create dataframe to store losses\n","loss_df = pd.DataFrame(columns=[j for j in range(1, pred_length+1)])\n","loss_unscaled_df = pd.DataFrame(columns=[j for j in range(1, pred_length+1)])\n","loss_mape_df = pd.DataFrame(columns=[j for j in range(1, pred_length+1)])\n","\n","true_values = []\n","\n","for i in range(dataset.shape[0] - window_size - pred_length+1):\n","  # Get first query for each sequence\n","  query = dataset[i:i+window_size].reshape(dataset.shape[1]*window_size)\n","  # print(query)\n","  for j in range(pred_length):\n","    # Predict\n","    pred = fcnn_model(query.reshape(1,-1)).detach()\n","\n","    # Extract true value + additional data from next row\n","    next_row = dataset[i+j+window_size:i+j+window_size+1]\n","    true_value = next_row[:,-1]\n","    additional_data = next_row[:,:-1][0]\n","\n","    # Compute loss and add to dataframes\n","    loss = mean_squared_error(pred.detach(), true_value.detach())\n","    loss_unscaled = mean_squared_error(target_scaler.inverse_transform(pred.detach()), target_scaler.inverse_transform(true_value.detach()))\n","    loss_mape = mean_absolute_percentage_error(target_scaler.inverse_transform(true_value.detach()), target_scaler.inverse_transform(pred.detach()))\n","    loss_df.loc[i, j+1] = loss\n","    loss_unscaled_df.loc[i, j+1] = loss_unscaled\n","    loss_mape_df.loc[i, j + 1] = loss_mape\n","    additional_data = torch.squeeze(additional_data,-1)\n","    pred = torch.squeeze(pred, -1)\n","    # Create new query window from data + prediction\n","    new_row = torch.cat((additional_data, pred),0)\n","    query = torch.cat((query[dataset.shape[1]:], new_row), 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vchVwoIhrzeU"},"source":["##### Test"]},{"cell_type":"code","metadata":{"id":"hw09N7virzeV"},"source":["print('Sequence RMSE: {:.3f}'.format(sqrt(loss_unscaled_df.mean().mean())))\n","print('Sequence MAPE: {:.4f}'.format(loss_mape_df.mean().mean()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6-y2-EHrzeW"},"source":["# GET SQUAREROOT OF VALUES\n","# CURRENTLY REPORTING MSE BELOW\n","# Plot results\n","plt.title('FCNN autoregressive error')\n","plt.ylabel('RMSE')\n","plt.xlabel('Forecasted time-steps')\n","plt.plot(loss_unscaled_df.mean())\n","plt.show()"],"execution_count":null,"outputs":[]}]}